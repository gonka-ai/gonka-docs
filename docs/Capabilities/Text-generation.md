# Text generation models

Text generation models, also referred to as large language models or generative pre-trained transformers, are designed to understand and produce natural language and code. These models create text outputs based on the inputs they are given, often known as “prompts.” Structuring a prompt effectively is similar to “programming” the model by providing it with instructions or examples that outline the desired task.

By leveraging open-source text generation models, you can develop applications for various purposes, such as:

- Composing documents
- Generating computer code
- Responding to questions based on a knowledge base
- Analyzing textual content
- Enabling natural language interfaces for software
- Providing tutoring across different subjects
- Translating languages
- Simulating characters for interactive environments

To use a model via the WeAI API, you submit a request through the Chat Completions API with your inputs and API key, receiving a response containing the model’s output.

You can experiment with various models using the chat playground.

## Quickstart
Chat models process a series of messages as input and produce a response generated by the model as output. While the chat structure is optimized for multi-turn interactions, it is equally effective for single-turn tasks that do not require ongoing dialogue.

An example call to the [Chat Completions](Endpoints/chatcompletions.md) API would look like this:

=== "python"
    ```
    from weai import WeAI
    client = WeAI()

    response = client.chat.completions.create(
    model="llama3.1-8B",
    messages=[
        {"role": "system", "content": "You are a helpful assistant."},
        {"role": "user", "content": "Who won the world series in 2020?"},
        {"role": "assistant", "content": "The Los Angeles Dodgers won the World Series in 2020."},
        {"role": "user", "content": "Where was it played?"}
    ]
    )
    ```

=== "curl"
    ```
    curl https://weai.productscience.ai/v1/chat/completions \
        -H "Content-Type: application/json" \
        -H "Authorization: Bearer $WEAI_API_KEY" \
        -d '{
            "model": "llama3.1-8B",
            "messages": [
            {
                "role": "system",
                "content": "You are a helpful assistant."
            },
            {
                "role": "user",
                "content": "Who won the world series in 2020?"
            },
            {
                "role": "assistant",
                "content": "The Los Angeles Dodgers won the World Series in 2020."
            },
            {
                "role": "user",
                "content": "Where was it played?"
            }
            ]
        }'
    ```    
    
=== "node.js"
    ```
    import WeAI from "weai";
    const weai = new WeAI();
    async function main() {
    const completion = await weai.chat.completions.create({
    messages: [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "What is a LLM?"}
    ],
    model: "llama3.1-8B",
    });
    console.log(completion.choices[0]);
    }
    main();
    ```

To learn more, you can view the [Chat Completions](Endpoints/chatcompletions) guide.

## Prompt engineering

Understanding the best practices for using LLMs is crucial for optimizing application performance. Each model has specific failure modes, and knowing how to mitigate or address these issues is not always straightforward. The field of working with language models, originally called “prompt engineering,” has evolved beyond just designing prompts; it now encompasses developing systems that integrate model queries as components.

For further details, explore our guide on prompt engineering, which explains techniques to enhance model reasoning, minimize the chances of hallucinations, and more.

## FAQ
### Which model should I choose?

We typically suggest defaulting to llama3.1-8B.
If your application requires advanced intelligence, consider evaluating llama3.1-70B.

You can use the playground to experiment and find the best balance between price and performance for your needs. A common approach is to employ multiple query types, directing each to the model best suited for the task.

### How should I configure the temperature parameter?

The temperature parameter controls randomness, with 0 being the most deterministic and 2 being the least.

Setting a low temperature (e.g., 0.2) results in more consistent but potentially robotic responses. Higher values (e.g., above 1.0 and approaching 2.0) may cause the model’s output to become less predictable. For creative outputs, using a temperature around 1.2 and a prompt that explicitly encourages creativity can be effective, but experimentation is recommended.

### Is fine-tuning supported for the latest models?

Refer to the fine-tuning guide for the most up-to-date information on which models support fine-tuning and how to begin.

### How can I enhance the safety of my application?

To implement a moderation layer for Chat API outputs, you can follow our moderation guide, which helps filter content that may violate WeAI’s usage policies. We also recommend reviewing our safety guide for additional strategies to create safer systems.
