# Models

## Flagship model
### llama3.1 405B
The flagship 405B model of Llama 3.1 boasts an impressive scale, designed for high-performance multilingual text generation and comprehension. With 405 billion parameters, it sets new benchmarks for quality and contextual accuracy in language tasks. This model is particularly effective in environments requiring nuanced understanding and generation, making it ideal for advanced applications in research and industry. Its extensive training on diverse data allows it to handle a wide range of languages and complex queries seamlessly.

## Models overview
The WeAI API is powered by a diverse set of open-source models, including LLaMA 3.1 (8B, 70B, and 405B) and Mistral AI. These models offer a range of capabilities to support various applications.

## Models	Description

### llama3.1 8B
The Llama 3.1 8B model is a lightweight, multilingual auto-regressive language model optimized for diverse dialogue applications. With 8 billion parameters, it leverages advanced training techniques like supervised fine-tuning and reinforcement learning from human feedback to deliver high-quality responses. Ideal for assistant-like tasks, it supports multiple languages including English, Spanish, and German, making it versatile for various commercial and research use cases.

### llama3.1 70B
The 70B variant of Llama 3.1 offers a significant increase in capacity, providing enhanced contextual understanding and generation capabilities. With 70 billion parameters, this model excels in complex dialogue scenarios and is well-suited for applications requiring deeper conversational engagement. Its multilingual support ensures it can cater to a global audience, making it a robust choice for businesses and researchers aiming to develop sophisticated language-based solutions.

### llama3.1 405B
The flagship 405B model of Llama 3.1 boasts an impressive scale, designed for high-performance multilingual text generation and comprehension. With 405 billion parameters, it sets new benchmarks for quality and contextual accuracy in language tasks. This model is particularly effective in environments requiring nuanced understanding and generation, making it ideal for advanced applications in research and industry. Its extensive training on diverse data allows it to handle a wide range of languages and complex queries seamlessly.

### Mistral Large 2
The Mistral Large 2 model is engineered for high-complexity tasks, delivering top-tier reasoning capabilities. This multilingual model supports a wide range of languages, including European languages, Chinese, Japanese, Korean, Hindi, and Arabic. With an impressive context window of 128K tokens, it excels in handling extensive inputs. Additionally, it features native function calling and JSON output capabilities, making it highly effective for advanced coding tasks across 80+ programming languages.

### Mistral Small 24.09
The Mistral Small 24.09 model stands out as an enterprise-grade solution in the small model category, renowned for its power and efficiency. Available under the Mistral Research License, it offers a 128K token context window, allowing for comprehensive processing of input data. This cost-effective and fast model is well-suited for a variety of applications, including translation, summarization, and sentiment analysis, making it an excellent choice for diverse use cases.

### Mistral NeMo
Mistral NeMo is a cutting-edge 12B small model developed in collaboration with NVIDIA, setting new standards in its size category. It supports multiple languages, including European languages, Chinese, Japanese, Korean, Hindi, and Arabic. With a large context window of 128K tokens, this model excels in complex tasks while maintaining high performance. Licensed under Apache 2.0, Mistral NeMo is designed for versatility and robustness, making it ideal for a broad spectrum of applications.

### Continuous model upgrades
`Llama 3.1 (8B)`, `Llama 3.1 (70B)`, `Llama 3.1 (405B)`, `Mistral Large 2`, `Mistral Small 24.09`, and `Mistral NeMo` point to their respective latest model version. You can verify this by looking at the response object after sending a request. The response will include the specific model version used (e.g. `mistral-large-2407`).