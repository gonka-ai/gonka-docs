#WeAI developer platform

WeAI is a decentralized AI infrastructure designed to optimize computational power specifically for AI model training and inference, offering a competitive alternative to traditional centralized cloud providers. Centralized systems are often expensive, monopolistic, and carry risks of censorship while existing decentralized networks frequently waste resources on non-productive tasks like network security.

WeAI platform introduces an innovative consensus mechanism that ensures nearly **100%** of computational resources are used for meaningful AI tasks, maximizing efficiency and minimizing operational costs. By democratizing access to advanced AI capabilities, WeAI empowers developers and businesses with cost-effective, flexible, and transparent solutions free from the limitations and control of centralized providers.

The system features key roles: 
- **developers** build and deploy AI applications using the network’s distributed power.
- **hardware providers** (or "participants") contribute computational resources and earn rewards based on their input; 

This collaboration allows the platform to offer AI services at significantly lower prices, making advanced AI technology more accessible to a wider audience.

WeAI features key roles:

- **developers** build and deploy AI applications using the network’s distributed power;
- **hardware providers** (participants) contribute computational resources and earn rewards based on their input;

This collaboration allows the platform to offer AI services at significantly lower prices, making advanced AI technology more accessible to a wider audience.

## Meet the models

Here is a selection of open-source models, including LLaMA 3.1 in its various configurations—8B, 70B—alongside Mistral AI. These models provide diverse capabilities suitable for a wide range of applications, from lightweight tasks to complex analyses.

- [llama3.1 8B](Capabilities/Models.md): Light-weight, ultra-fast model you can run anywhere.
- [llama3.1 70B](Capabilities/Models.md): Highly performant, cost-effective model that enables diverse use cases.
- [Mistral Large 2](Capabilities/Models.md): Top-tier reasoning for high-complexity tasks for your most sophisticated needs.
- [Mistral Small 24.09](Capabilities/Models.md): Enterprise-grade small model.
